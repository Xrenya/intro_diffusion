{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Uq5Uor90uDid"
      },
      "outputs": [],
      "source": [
        "from typing import Optional\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from IPython.display import HTML\n",
        "from matplotlib.animation import FuncAnimation, PillowWriter\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import transforms\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "67ukcH8D6LkF"
      },
      "outputs": [],
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, images: str, labels: str, transforms):\n",
        "        self.images = np.load(images, allow_pickle=True)\n",
        "        self.labels = np.load(labels, allow_pickle=True)\n",
        "        print(\"Images' shape: (%d, %d, %d, %d)\" % self.images.shape)\n",
        "        print(\"Labels' shape: (%d, %d)\" % self.labels.shape)\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.images.shape[0]\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        image = self.images[index]\n",
        "        label = torch.tensor(self.labels[index], dtype=torch.int64)\n",
        "        if self.transforms:\n",
        "            image = self.transforms(image)\n",
        "        return (image, label)\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "9ccS3sYg-HMB"
      },
      "outputs": [],
      "source": [
        "class ResidualConvBlock(nn.Module):\n",
        "    def __init__(self, in_channels: int, out_channels: int, skip: bool = False) -> None:\n",
        "        super().__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.skip = skip\n",
        "\n",
        "        if self.in_channels != self.out_channels:\n",
        "            self.shortcut = nn.Conv2d(\n",
        "                in_channels=in_channels,\n",
        "                out_channels=out_channels,\n",
        "                kernel_size=1,\n",
        "                stride=1,\n",
        "                padding=0,\n",
        "            )\n",
        "        else:\n",
        "            self.shortcut = nn.Identity()\n",
        "\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=in_channels,\n",
        "                out_channels=out_channels,\n",
        "                kernel_size=3,\n",
        "                stride=1,\n",
        "                padding=1,\n",
        "            ),  # same size\n",
        "            nn.BatchNorm2d(num_features=out_channels),\n",
        "            nn.GELU()\n",
        "        )\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=out_channels,\n",
        "                out_channels=out_channels,\n",
        "                kernel_size=3,\n",
        "                stride=1,\n",
        "                padding=1,\n",
        "            ),  # same size\n",
        "            nn.BatchNorm2d(num_features=out_channels),\n",
        "            nn.GELU(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x1 = self.conv1(x)\n",
        "        out = self.conv2(x1)\n",
        "        if self.skip:\n",
        "            out = self.shortcut(x) + out\n",
        "        return out\n",
        "\n",
        "\n",
        "class UnetDown(nn.Module):\n",
        "    def __init__(self, in_channels: int, out_channels: int) -> None:\n",
        "        super().__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            ResidualConvBlock(in_channels=in_channels, out_channels=out_channels),\n",
        "            ResidualConvBlock(in_channels=out_channels, out_channels=out_channels),\n",
        "            nn.MaxPool2d(2),\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return self.layers(x)\n",
        "\n",
        "\n",
        "class UnetUp(nn.Module):\n",
        "    def __init__(self, in_channels: int, out_channels: int) -> None:\n",
        "        super().__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Upsample(scale_factor=2, mode=\"nearest\"),\n",
        "            nn.Conv2d(\n",
        "                in_channels=in_channels,\n",
        "                out_channels=out_channels,\n",
        "                kernel_size=1,\n",
        "                stride=1,\n",
        "                padding=0,\n",
        "            ),\n",
        "            ResidualConvBlock(in_channels=out_channels, out_channels=out_channels),\n",
        "            ResidualConvBlock(in_channels=out_channels, out_channels=out_channels),\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor, skip_tensor: torch.Tensor) -> torch.Tensor:\n",
        "        x = torch.cat([x, skip_tensor], dim=1)\n",
        "\n",
        "        x = self.layers(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class EmbedFC(nn.Module):\n",
        "    def __init__(self, input_dim: int, embed_dim: int) -> None:\n",
        "        super().__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(input_dim, embed_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(embed_dim, embed_dim),\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = x.view(-1, self.input_dim)\n",
        "        return self.layers(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "05aJ9E379f2T"
      },
      "outputs": [],
      "source": [
        "class ContextUnet(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels: int,\n",
        "        hidden_dim: int = 256,\n",
        "        context_feature_dim: int = 10,\n",
        "        size: int = 28,\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.context_feature_dim = context_feature_dim\n",
        "        self.size = size  #assume h == w. must be divisible by 4, so 28,24,20,16...\n",
        "\n",
        "        self.init_conv = ResidualConvBlock(\n",
        "            in_channels=in_channels,\n",
        "            out_channels=hidden_dim,\n",
        "            skip=True\n",
        "        )\n",
        "\n",
        "        self.down1 = UnetDown(hidden_dim, hidden_dim)   # down1 #[10, 256, 8, 8]\n",
        "        self.down2 = UnetDown(hidden_dim, 2 * hidden_dim)  # down2 #[10, 256, 4,  4]\n",
        "\n",
        "        # original: self.to_vec = nn.Sequential(nn.AvgPool2d(7), nn.GELU())\n",
        "        self.to_vec = nn.Sequential(\n",
        "            nn.AvgPool2d(4),\n",
        "            nn.GELU(),\n",
        "        )\n",
        "\n",
        "        # embed timestep\n",
        "        self.time_embed_1 = EmbedFC(1, 2 * hidden_dim)\n",
        "        self.time_embed_2 = EmbedFC(1, 1 * hidden_dim)\n",
        "\n",
        "        # embed context label\n",
        "        self.context_embed_1 = EmbedFC(context_feature_dim, 2 * hidden_dim)\n",
        "        self.context_embed_2 = EmbedFC(context_feature_dim, hidden_dim)\n",
        "\n",
        "        self.up0 = nn.Sequential(\n",
        "            nn.Upsample(scale_factor=4),\n",
        "            nn.Conv2d(\n",
        "                in_channels=2 * hidden_dim,\n",
        "                out_channels=2 * hidden_dim,\n",
        "                kernel_size=1,\n",
        "                stride=1,\n",
        "                padding=0,\n",
        "            ),\n",
        "            nn.GroupNorm(\n",
        "                num_groups=8,\n",
        "                num_channels=2 * hidden_dim\n",
        "            ),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "        self.up1 = UnetUp(\n",
        "            in_channels=4 * hidden_dim,\n",
        "            out_channels=hidden_dim\n",
        "        )\n",
        "        self.up2 = UnetUp(\n",
        "            in_channels=2 * hidden_dim,\n",
        "            out_channels=hidden_dim\n",
        "        )\n",
        "\n",
        "        self.out = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=2 * hidden_dim,\n",
        "                out_channels=hidden_dim,\n",
        "                kernel_size=3,\n",
        "                stride=1,\n",
        "                padding=1\n",
        "            ),\n",
        "            nn.GroupNorm(\n",
        "                num_groups=8,\n",
        "                num_channels=hidden_dim\n",
        "            ),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(\n",
        "                in_channels=hidden_dim,\n",
        "                out_channels=self.in_channels,\n",
        "                kernel_size=3,\n",
        "                stride=1,\n",
        "                padding=1,\n",
        "            ),\n",
        "        )\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        image: torch.Tensor,\n",
        "        timesteps: torch.Tensor,\n",
        "        context_label: Optional[None | torch.Tensor] = None,\n",
        "    ) -> torch.Tensor:\n",
        "        x = self.init_conv(image)\n",
        "\n",
        "        down1 = self.down1(x)\n",
        "        down2 = self.down2(down1)\n",
        "\n",
        "        latent = self.to_vec(down2)\n",
        "\n",
        "        if context_label is None:\n",
        "            context_label = torch.zeros(x.shape[0], self.context_feature_dim).to(x.device)\n",
        "        # context\n",
        "        context_embed_1 = self.context_embed_1(context_label).view(-1, self.hidden_dim * 2, 1, 1)\n",
        "\n",
        "        context_embed_2 = self.context_embed_2(context_label).view(-1, self.hidden_dim, 1, 1)\n",
        "        # timesteps\n",
        "        time_embed_1 = self.time_embed_1(timesteps).view(-1, self.hidden_dim * 2, 1, 1)\n",
        "        time_embed_2 = self.time_embed_2(timesteps).view(-1, self.hidden_dim, 1, 1)\n",
        "\n",
        "        up1 = self.up0(latent)\n",
        "        up2 = self.up1(context_embed_1 * up1 + time_embed_1, down2)\n",
        "        up3 = self.up2(context_embed_2 * up2 + time_embed_2, down1)\n",
        "\n",
        "        out = self.out(torch.cat([up3, x], dim=1))\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "phVwo_eX9psx"
      },
      "outputs": [],
      "source": [
        "timesteps = 500\n",
        "beta_start = 1e-4\n",
        "beta_end = 0.02\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "hidden_dim = 64\n",
        "context_dim = 5\n",
        "size = 16\n",
        "batch_size = 100\n",
        "n_epoch = 32\n",
        "lr = 1e-3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "pP2-niZt5Q6T"
      },
      "outputs": [],
      "source": [
        "def denormalize_and_clip(image: torch.Tensor) -> torch.Tensor:\n",
        "    image *= 0.5\n",
        "    image += 0.5\n",
        "    return image.clip(0, 1)\n",
        "\n",
        "\n",
        "class NoiseScheduler(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        timesteps: int = 24,\n",
        "        beta_start: float = 1e-4,\n",
        "        beta_end: float = 0.6\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.timesteps = timesteps\n",
        "        self.beta_start = beta_start\n",
        "        self.beta_end = beta_end\n",
        "\n",
        "        beta = torch.linspace(beta_start, beta_end, timesteps)\n",
        "        alpha = 1. - beta\n",
        "        alpha_bar = torch.cumprod(alpha, 0)\n",
        "\n",
        "        self.register_buffer('beta', beta)\n",
        "        self.register_buffer('alpha', alpha)\n",
        "        self.register_buffer('alpha_bar', alpha_bar)\n",
        "\n",
        "    def add_noise(self, x: torch.Tensor, t: torch.Tensor, noise: torch.Tensor):\n",
        "        \"\"\"\n",
        "        Adds a single step of noise\n",
        "        :param x: image we are adding noise to\n",
        "        :param t: step number, 0 indexed (0 <= t < steps)\n",
        "        :return: image with noise added\n",
        "        \"\"\"\n",
        "        alpha_bar = self.alpha_bar[t].view(-1, 1, 1, 1)\n",
        "        return torch.sqrt(alpha_bar) * x + torch.sqrt(1 - alpha_bar) * noise\n",
        "\n",
        "    def sample_prev_step(self, xt, t, pred_noise):\n",
        "        z = torch.randn_like(xt)\n",
        "        z[t.expand_as(z) == 0] = 0\n",
        "\n",
        "        mean = (1 / torch.sqrt(self.alpha[t])) * (xt - (self.beta[t] / torch.sqrt(1 - self.alpha_bar[t])) * pred_noise)\n",
        "        var = ((1 - self.alpha_bar[t - 1])  / (1 - self.alpha_bar[t])) * self.beta[t]\n",
        "        sigma = torch.sqrt(var)\n",
        "\n",
        "        x = mean + sigma * z\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "aEq1VOVe54C_"
      },
      "outputs": [],
      "source": [
        "noise_scheduler = NoiseScheduler(timesteps, beta_start, beta_end).to(device)\n",
        "\n",
        "nn_model = ContextUnet(\n",
        "    in_channels=3,\n",
        "    hidden_dim=hidden_dim,\n",
        "    context_feature_dim=context_dim,\n",
        "    size=size\n",
        ").to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ja17TCx54um",
        "outputId": "cfd0748a-490c-45d1-aaeb-f2d4111d649a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Images' shape: (89400, 16, 16, 3)\n",
            "Labels' shape: (89400, 5)\n"
          ]
        }
      ],
      "source": [
        "dataset = CustomDataset(\"sprites_1788_16x16.npy\", \"sprite_labels_nc_1788_16x16.npy\", transforms=transform)\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=1)\n",
        "optim = torch.optim.AdamW(nn_model.parameters(), lr=lr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 627
        },
        "id": "ORML8Vtg_FTj",
        "outputId": "b5077e67-31b2-4f85-fbf6-ad4f163a8e7b"
      },
      "outputs": [],
      "source": [
        "criterion = nn.MSELoss()\n",
        "nn_model.train()\n",
        "\n",
        "for epoch in range(n_epoch):\n",
        "\n",
        "    # Decay learning rate\n",
        "    optim.param_groups[0][\"lr\"] = lr * (1 - epoch / n_epoch)\n",
        "\n",
        "    pbar = tqdm(dataloader, mininterval=2)\n",
        "    losses = []\n",
        "    for x, _ in pbar:\n",
        "        optim.zero_grad()\n",
        "\n",
        "        x = x.to(device)\n",
        "\n",
        "        noise = torch.randn_like(x)\n",
        "        t = torch.randint(1, timesteps, (x.shape[0],), device=device)\n",
        "        x_pert = noise_scheduler.add_noise(x, t, noise)\n",
        "\n",
        "        pred_noise = nn_model(x_pert, t / timesteps)\n",
        "\n",
        "        loss = criterion(pred_noise, noise)\n",
        "        loss.backward()\n",
        "\n",
        "        optim.step()\n",
        "\n",
        "        losses.append(loss.detach().cpu().item())\n",
        "    if epoch % 4 == 0 or epoch == n_epoch - 1:\n",
        "        print(f\"Epoch {epoch} - Loss {np.mean(losses)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "_tZ_sEGPCy4c"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def sample_ddpm(n_sample: int, channels: int = 3, size: int = 16, save_rate: int = 20):\n",
        "    # x_T ~ N(0, 1), sample initial noise\n",
        "    samples = torch.randn(n_sample, channels, size, size).to(device)\n",
        "\n",
        "    # array to keep track of generated steps for plotting\n",
        "    intermediate = []\n",
        "    for i in range(timesteps - 1, 0, -1):\n",
        "        print(f'Sampling timestep {i:3d}', end='\\r')\n",
        "\n",
        "        # reshape time tensor\n",
        "        t = torch.tensor([i / timesteps])[:, None, None, None].to(device)\n",
        "\n",
        "        eps = nn_model(samples, t)\n",
        "        tensor_i = torch.tensor(i, device=device).view(1,)\n",
        "        samples = noise_scheduler.sample_prev_step(samples, tensor_i, eps)\n",
        "        if i % save_rate ==0 or i == timesteps or i<8:\n",
        "            intermediate.append(samples.detach().cpu().numpy())\n",
        "\n",
        "    intermediate = np.stack(intermediate)\n",
        "    return samples, intermediate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "TplGEGn2NPY-"
      },
      "outputs": [],
      "source": [
        "def plot_sample(x_gen_store,n_sample,nrows,save_dir, fn):\n",
        "    ncols = n_sample // nrows\n",
        "    sx_gen_store = np.moveaxis(x_gen_store, 2, 4)  # change to Numpy image format (h,w,channels) vs (channels,h,w)\n",
        "    nsx_gen_store = denormalize_and_clip(sx_gen_store)\n",
        "    # create gif of images evolving over time, based on x_gen_store\n",
        "    fig, axs = plt.subplots(\n",
        "        nrows=nrows,\n",
        "        ncols=ncols,\n",
        "        sharex=True,\n",
        "        sharey=True,\n",
        "        figsize=(ncols, nrows)\n",
        "    )\n",
        "    def animate_diff(i, store):\n",
        "        print(f'gif animating frame {i} of {store.shape[0]}', end='\\r')\n",
        "        plots = []\n",
        "        for row in range(nrows):\n",
        "            for col in range(ncols):\n",
        "                axs[row, col].clear()\n",
        "                axs[row, col].set_xticks([])\n",
        "                axs[row, col].set_yticks([])\n",
        "                plots.append(axs[row, col].imshow(store[i, (row * ncols) + col]))\n",
        "        return plots\n",
        "    ani = FuncAnimation(\n",
        "        fig,\n",
        "        animate_diff,\n",
        "        fargs=[nsx_gen_store],\n",
        "        interval=200,\n",
        "        blit=False,\n",
        "        repeat=True,\n",
        "        frames=nsx_gen_store.shape[0]\n",
        "    )\n",
        "    plt.close()\n",
        "    ani.save(save_dir + f\"{fn}.gif\", dpi=100, writer=PillowWriter(fps=5))\n",
        "    return ani"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 514
        },
        "id": "WjDhs0zVC458",
        "outputId": "4c0f8a5d-d57b-4ced-fcd7-5d75c5e861d8"
      },
      "outputs": [],
      "source": [
        "nn_model.eval()\n",
        "\n",
        "plt.clf()\n",
        "samples, intermediate_ddpm = sample_ddpm(32)\n",
        "save_dir = \"/content/\"\n",
        "animation_ddpm = plot_sample(intermediate_ddpm, 32, 4, save_dir, \"ani_run\")\n",
        "HTML(animation_ddpm.to_jshtml())"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
